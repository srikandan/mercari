{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "altered-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from num2words import num2words\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fossil-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontractions(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = decontractions(text)\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', ' ', text)\n",
    "    text = [t for t in text if t not in string.punctuation]\n",
    "    return ''.join(text)\n",
    "\n",
    "def removeStopWords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in list(stop_words)] \n",
    "    filtered_sentence = ' '.join(filtered_sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "def lemmatizer(text, spacydata):\n",
    "    lem_text = spacydata(text)\n",
    "    lem_text = [i.lemma_ for i in lem_text]\n",
    "    return ' '.join(lem_text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pat, '', text)\n",
    "\n",
    "def num_to_word(text):\n",
    "    digits = [int(s) for s in text.split() if s.isdigit()]\n",
    "    digits_map = {str(i): num2words(i) for i in digits}\n",
    "    for i in digits_map.keys():\n",
    "        text = text.replace(i, digits_map[i])\n",
    "    return text\n",
    "\n",
    "def remove_extra_whitespace_tabs(text):\n",
    "    pattern = r'^\\s*|\\s\\s*'\n",
    "    return re.sub(pattern, ' ', text).strip()\n",
    "\n",
    "def contains_bundle(data):\n",
    "    if (('bundles' in data) or ('bundle' in data)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_sentiment_score(data):\n",
    "    senti = SentimentIntensityAnalyzer()\n",
    "    if data != 'miss':\n",
    "        sentence_sentiment_score = senti.polarity_scores(data)\n",
    "        compound = sentence_sentiment_score['compound']\n",
    "        if compound >= 0.5:\n",
    "            return 3 \n",
    "        if compound >= (-0.5) and compound < 0.5:\n",
    "            return 2\n",
    "        if compound < (-0.5):\n",
    "            return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "independent-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_data(data, do_preprocess=False, spacydata=None):\n",
    "    if (type(data) == str):\n",
    "        data = data.strip()\n",
    "    if (data == '' or pd.isna(data) or data.lower() == 'no description yet'):\n",
    "        data = 'miss'\n",
    "    elif (do_preprocess):\n",
    "        data = preprocess(data)\n",
    "        data = remove_special_characters(data)\n",
    "        data = num_to_word(data)\n",
    "        data = remove_extra_whitespace_tabs(data)\n",
    "        data = lemmatizer(data, spacydata)\n",
    "    else:\n",
    "        data = data.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "delayed-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data, encoder):\n",
    "    return encoder.transform(np.array(data).reshape(-1, 1))\n",
    "\n",
    "def tokanize_data(data, tokanizer):\n",
    "    return np.array(tokanizer.texts_to_sequences([data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "directed-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_predict_data(data):\n",
    "    x = {\n",
    "        'name': pad_sequences(data['name'], maxlen=30),\n",
    "        'item_description': pad_sequences(data['item_description'], maxlen=70),\n",
    "        'item_condition_id': np.array(data['item_condition_id']).reshape(1, -1), \n",
    "        'brand_name': np.array(data['brand_name']).reshape(1, -1), \n",
    "        'sub_l1': np.array(data['sub_l1']).reshape(1, -1), \n",
    "        'sub_l2': np.array(data['sub_l2']).reshape(1, -1), \n",
    "        'sub_l3': np.array(data['sub_l3']).reshape(1, -1), \n",
    "        'shipping': np.array(data['shipping']).reshape(1, -1), \n",
    "        'contains_bundle': np.array(data['contains_bundle']).reshape(1, -1),\n",
    "        'item_description_score': np.array(data['item_description_score']).reshape(1, -1), \n",
    "    }\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finite-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    predicted = model.predict(data)\n",
    "    predicted = predicted.flatten().tolist()\n",
    "    temp = []\n",
    "    for i in predicted:\n",
    "        if i<0:\n",
    "            temp.append(0.0)\n",
    "        else:\n",
    "            temp.append(i)\n",
    "\n",
    "    predicted = temp\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dirty-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(data):\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    model = tf.keras.models.load_model('/Applied AI/Assignments/29.SelfCase Study - 1/Best Model/mercari_lbl_cnn_lstm_model.h5', compile=False)\n",
    "    \n",
    "    with open(\"/Applied AI/Assignments/29.SelfCase Study - 1/Best Model/mercari_category_encoder.pkl\", \"rb\") as fp:\n",
    "        category_encoder = pickle.load(fp)\n",
    "        \n",
    "    with open(\"/Applied AI/Assignments/29.SelfCase Study - 1/Best Model/mercari_brand_encoder.pkl\", \"rb\") as fp:\n",
    "        brand_encoder = pickle.load(fp)\n",
    "        \n",
    "    with open(\"/Applied AI/Assignments/29.SelfCase Study - 1/Best Model/mercari_name_desc_tokenizer.pkl\", \"rb\") as fp:\n",
    "        name_desc_tokenizer = pickle.load(fp)\n",
    "        \n",
    "    pre_data = data.copy()\n",
    "    \n",
    "    pre_data['name'] = get_preprocessed_data(pre_data['name'], True, sp)\n",
    "    pre_data['item_description'] = get_preprocessed_data(pre_data['item_description'], True, sp)\n",
    "    pre_data['brand_name'] = get_preprocessed_data(pre_data['brand_name'])\n",
    "    pre_data['sub_l1'] = get_preprocessed_data(pre_data['sub_l1'])\n",
    "    pre_data['sub_l2'] = get_preprocessed_data(pre_data['sub_l2'])\n",
    "    pre_data['sub_l3'] = get_preprocessed_data(pre_data['sub_l3'])\n",
    "    pre_data['contains_bundle'] = contains_bundle(pre_data['name'] + ' ' + pre_data['item_description'])\n",
    "    pre_data['item_description_score'] = get_sentiment_score(pre_data['item_description'])\n",
    "    \n",
    "    pre_data['sub_l1'] = encode_data(pre_data['sub_l1'], category_encoder)\n",
    "    pre_data['sub_l2'] = encode_data(pre_data['sub_l2'], category_encoder)\n",
    "    pre_data['sub_l3'] = encode_data(pre_data['sub_l3'], category_encoder)\n",
    "    pre_data['brand_name'] = encode_data(pre_data['brand_name'], brand_encoder)\n",
    "    \n",
    "    pre_data['name'] = tokanize_data(pre_data['name'], name_desc_tokenizer)\n",
    "    pre_data['item_description'] = tokanize_data(pre_data['item_description'], name_desc_tokenizer)\n",
    "    \n",
    "    pre_data = form_predict_data(pre_data)\n",
    "    \n",
    "    predicted_result = predict(pre_data, model)[0]\n",
    "    \n",
    "    print('Predicted price of the product is ','\"', round(predicted_result, 2),'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crucial-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Dataset/train.tsv', sep='\\t')\n",
    "given_data = df.head(1)\n",
    "given_data = given_data.to_dict('index')[0]\n",
    "given_data['sub_l1'] = 'Men'\n",
    "given_data['sub_l2'] = 'shoes'\n",
    "given_data['sub_l3'] = 'athletic training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "buried-latter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_id': 0,\n",
       " 'name': 'MLB Cincinnati Reds T Shirt Size XL',\n",
       " 'item_condition_id': 3,\n",
       " 'category_name': 'Men/Tops/T-shirts',\n",
       " 'brand_name': nan,\n",
       " 'price': 10.0,\n",
       " 'shipping': 1,\n",
       " 'item_description': 'No description yet',\n",
       " 'sub_l1': 'Men',\n",
       " 'sub_l2': 'shoes',\n",
       " 'sub_l3': 'athletic training'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incident-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of the product is  \" 15.34 \"\n",
      "Time taken to execute: 3.6\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "predict_data(given_data)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken to execute:', round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "angry-webcam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_id': 0,\n",
       " 'name': 'puma wear',\n",
       " 'item_condition_id': 1,\n",
       " 'category_name': 'Men/Tops/T-shirts',\n",
       " 'brand_name': 'puma',\n",
       " 'price': 10.0,\n",
       " 'shipping': 0,\n",
       " 'item_description': 'No description yet',\n",
       " 'sub_l1': 'Men',\n",
       " 'sub_l2': 'shoes',\n",
       " 'sub_l3': 'athletic training'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_data['name'] = 'puma wear'\n",
    "given_data['item_condition_id'] = 1\n",
    "given_data['brand_name'] = 'puma'\n",
    "given_data['shipping'] = 0\n",
    "given_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dried-circle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of the product is  \" 21.68 \"\n",
      "Time taken to execute: 1.71\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "predict_data(given_data)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken to execute:', round(end - start, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
